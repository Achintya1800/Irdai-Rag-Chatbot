#!/usr/bin/env python3
"""
Run the updated pipeline with enhanced document link extraction
"""

import sys
import os
from loguru import logger

# Add src to path
sys.path.append(os.path.join(os.path.dirname(__file__), 'src'))

from scrapers.web_scraper import WebScraper
from utils.document_processor import DocumentProcessor
from rag.vector_db import VectorDatabase
from rag.rag_system import RAGSystem

def main():
    """Run the complete updated pipeline"""
    
    print("ğŸš€ Starting Updated IRDAI RAG Pipeline")
    print("=" * 50)
    
    # Step 1: Enhanced Web Scraping
    print("\nğŸ”„ Step 1: Enhanced Web Scraping (with document link extraction)")
    scraper = WebScraper()
    
    # Reset the database first
    try:
        vector_db = VectorDatabase()
        vector_db.reset_database()
        print("âœ… Database reset successfully")
    except Exception as e:
        print(f"âš ï¸ Database reset failed: {e}")
    
    # Scrape websites with enhanced document link extraction
    try:
        scraped_data = scraper.scrape_all_websites()
        scraper.save_scraped_data()
        
        # Show document link statistics
        total_doc_links = 0
        for data in scraped_data:
            total_doc_links += len(data.document_links)
        
        print(f"âœ… Successfully scraped {len(scraped_data)} pages")
        print(f"ğŸ“„ Total document links found: {total_doc_links}")
        
        # Show breakdown by site
        site_stats = {}
        for data in scraped_data:
            site = data.metadata.get('source_website', 'Unknown')
            if site not in site_stats:
                site_stats[site] = {'pages': 0, 'docs': 0}
            site_stats[site]['pages'] += 1
            site_stats[site]['docs'] += len(data.document_links)
        
        print("\nğŸ“Š Site-wise Statistics:")
        for site, stats in site_stats.items():
            print(f"  {site}: {stats['pages']} pages, {stats['docs']} documents")
        
    except Exception as e:
        print(f"âŒ Scraping failed: {e}")
        return False
    
    # Step 2: Document Processing
    print("\nğŸ”„ Step 2: Document Processing")
    try:
        processor = DocumentProcessor()
        chunks = processor.process_scraped_content(scraped_data)
        enriched_chunks = processor.enrich_metadata(chunks)
        
        print(f"âœ… Successfully processed {len(enriched_chunks)} document chunks")
        
        # Show document link preservation
        chunks_with_docs = sum(1 for chunk in enriched_chunks 
                             if chunk.metadata.get('document_links'))
        print(f"ğŸ“„ Chunks with document links: {chunks_with_docs}")
        
    except Exception as e:
        print(f"âŒ Document processing failed: {e}")
        return False
    
    # Step 3: Vector Database Building
    print("\nğŸ”„ Step 3: Vector Database Building")
    try:
        vector_db = VectorDatabase()
        vector_db.add_documents(enriched_chunks)
        
        stats = vector_db.get_collection_stats()
        print(f"âœ… Vector database built successfully")
        print(f"ğŸ“Š Total documents: {stats.get('total_documents', 0)}")
        print(f"ğŸ” Embedding model: {stats.get('embedding_model', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ Vector database building failed: {e}")
        return False
    
    # Step 4: Test RAG System
    print("\nğŸ”„ Step 4: Testing RAG System")
    try:
        rag_system = RAGSystem()
        
        # Test with a sample query
        test_query = "What are the latest IRDAI regulations?"
        print(f"ğŸ” Testing query: {test_query}")
        
        response = rag_system.query(test_query)
        
        print(f"âœ… RAG system test successful!")
        print(f"ğŸ“ Response length: {len(response.answer)} characters")
        print(f"ğŸ“Š Confidence score: {response.confidence_score:.2f}")
        print(f"ğŸ“š Sources found: {len(response.sources)}")
        
        # Count document links in response
        total_doc_links = 0
        for source in response.sources:
            if source.get('document_links'):
                total_doc_links += len(source['document_links'])
        
        print(f"ğŸ”— Document links in response: {total_doc_links}")
        
    except Exception as e:
        print(f"âŒ RAG system test failed: {e}")
        return False
    
    # Step 5: Final Statistics
    print("\nğŸ“Š Final Pipeline Statistics")
    print("=" * 50)
    print(f"âœ… Pipeline Status: SUCCESSFUL")
    print(f"ğŸŒ Websites Scraped: 4")
    print(f"ğŸ“„ Pages Scraped: {len(scraped_data)}")
    print(f"ğŸ“ Document Chunks: {len(enriched_chunks)}")
    print(f"ğŸ”— Total Document Links: {total_doc_links}")
    print(f"ğŸ—„ï¸ Vector Database Size: {stats.get('total_documents', 0)} documents")
    
    print("\nğŸ¯ Ready for Demo!")
    print("Run: streamlit run streamlit_app.py")
    
    return True

if __name__ == "__main__":
    success = main()
    if success:
        print("\nğŸ‰ Pipeline completed successfully!")
        print("ğŸ’¡ Your system is now ready for the manager presentation!")
    else:
        print("\nâŒ Pipeline failed. Please check the logs for details.")